# 우아콘 2023.11.15

## 오프닝 ~ 키노트

### CTO 송재하님의 키노트

"모든 일의 궁극적인 목적은 고객창출과 고객만족이다"

- AI 기술을 활용한 사례
- 배달로봇 Dilly

전체적으로 세션의 큰 줄기들을 소개 해주셔서 키노트로 적합하다는 생각이 들었다.

- 서버/백엔드 세션
- 클라이언트 (웹/앱) 세션
- 개발 생산성 제고
- 디자인 시스템 
- 보안/네트워크
- PM/소프트스킬/교육

### CPO 이기호님의 키노트

크게 3가지 경험에 관심을 갖고 발전시켜나가는 배달의 민족

배달경험 / 탐색 경험 / 다양한 경험 (음식주문 그 이상의 서비스)

전단지을 대체하면서 시작한 배달의 민족

한집배달에서의 고민과 해결

ML/AI를 활용한 좀 더 정확한 배달 시간 예측


MAU 2,000만 서비스
음식 주문으로 쌓은 노하우를 가지고 다양한 서비스 출시

- 배민 스토어
  - 반찬 / 애플 제품 / 홈플러스 등 오프라인 마켓
- 대용량 특가 서비스
- 배민 우리 동네

> 장관님의 축사 영상도 있어서 놀랬다



## 대규모 트랜잭션을 처리하는 배민 주문시스템 규모에 따른 진화

- 일평균 300만 주문건
- 점심시간 / 저녁시간에 피크를 찍는 독특한 트래픽 추이

- 단일 장애 포인트 -> MSA
- 대용량 데이터 성능 문제
  - 분리된 주문 DB 안에서 Query와 Command 모두 호출되고 있어 주문 시스템 성능에 영향을 줌
  - 분리된 테이블들의 데이터를 단일 도큐먼트로 집합
- 대규모 트랜잭션
  - 쓰기요청에 대한 대응은 DB 스케일업 외애 방법이 존재하지 않아서 어려움이 발생
  - Aurora에서 직접적으로 샤딩 지원이 안되니 애플리케이션 샤딩을 구성
  - Key Based Sharding (Hash Based Sharding)
    - 구현이 간단하면 샤드 클러스터 내 샤드들에 데이터를 골고루 분배할 수 있다
    - 장비를 동적으로 추가, 제거할때 데이터 재배치가 필요하다.
  - Range Based Sharding
    - 특정 값의 범위 기반으로 샤드를 결정하기 때문에 구현이 간단
    - 데이터가 균등하게 배분되지 않아 특정 샤으데 데이터가 몰릴 수 있음
  - Directory Based Sharding
    - 중간에 Lookup Table을 두는 방식
    - 샤드 결정 로직이 Look up 테이블로 분리되어 있어 동적으로 샤드 추가하는데 유리
    - Look up 테이블이 단일 장애 포인트가 될 수 있음
- "주문 시스템은 동적 주문 데이터를 최대 30일만 저장한다" 는 특징
  - 최대 30일까지만 주문 취소가 가능
  - 단일 장애 포인트는 피하면서
  - 샤드 추가 이후 30일이 지나면 데이터는 다시 균등하게 분배 된다는 결론 내림
- 결국 Key Based Sharding (Hash Based Sharding) 선택
  - 주문 순번 % 샤드 수 = 샤드번호
- 다건 조회시 데이터 애그리게이트
  - 분산된 샤드 데이터들을 어떻게 조회 목록에 노출시킬 것인가
  - MongoDB 에 이미 조회용 데이터를 보관하고 있어서 큰 문제 없이 가능
- 복잡한 이벤트 아키텍처
  - 규칙성 없는 무분별한 이벤트 발행
  - 내/외부 이벤트 분리
    - 주줌 도메인 이벤트는 내부 이벤트로, 서비스 로직은 외부 이벤트로 정의
    - 내부 이벤트는 ZERO Payload
  - 트랜잭션 내부 외부에서 발행 실패 유무에 따라 처리가 실패하게 됨
    - 트랜잭션 아웃박스 패턴 적용
    - 실패한 메세지 발행은 아웃박스 테이블에 담긴 데이터를 스프링 배치를 통해 재발행
  - 

## [11:55 - 12:30] 추천시스템 성장 일지 - 데이터 엔지니어편

현재 상황
- 30개 이상의 추천 서비스
- 하루 5천만건의 추천 API 호출

대표적인 시스템 구성 방법 
- 학습 + 오프라인 예측
  - S3에 저장된 주문,클릭,가게 정보를 S3에 저장하고 Hive를 통해 관리
  - 학습 데이터 생성 후 GPU를 통한 모델 학습 
- 학습 + 온라인 예측
  - 실시간 모델 추론
  - 모델뿐만 아니라 모델에 전달할 피쳐 데이터 필요
  - 스파크를 통해 피쳐 데이터 생성 후 MongDB에 데이터 업로드
  - 추천 API는 MongDB에 적재된 피쳐데이터를 활용해 예측을 진행
- 요구사항과 제약 사항에 따라 결정
  - 모델의 크기, 모델 예측속도, 입력값 경우의수, 실시간 정보 필요 여부 등에 따라 선택

추천 시스템 성장 일지
- 추천 시스템을 빠르게 성장시킬 수 있었던 결정들
- (1) 추천 전달 방식의 변화
 - API 보다 더 간단하게 구현할 수 있도록 서비스 백엔드와 추천시스템팀 사이에 중앙 DB를 두고 사용
 - 서비스 백엔드에서는 중앙 DB에 데이터를 적재하고, 추천 시스템은 추천결과를 저장하고, 다시 서비스 팀은 추천팀이 저장한 데이터를 불러와서 사용
 - 초반에 이러한 방법을 사용해서 개발 요소도 적고, 관리 포인트가 적어서 **운영 부담이 적다**.
 - 다만, 다른 팀에 대한 의존성, DB에 대한 의존성 등 문제를 겪게 됨
   - 새로운 컬럼을 추가하는 등의 작업이 있을 경우 반대편에서도 항상 대응을 같이 해줘야함
   - DB에 대한 의존성도 있어 DB 업데이트가 필요하거나 DB가 지원하지 않는 기능등은 당연히 사용할 수도 없음
 - 개선) API를 통한 커뮤니케이션으로 변경
   - 단, 꼭 필요한 데이터만 주고 받는다.
   - 추천 API 내부에서 어떻게 진행되는지는 바깥 서비스에서 알 필요가 없다.
   - 얻게 된 점
     - 외부 의존성이 최소화 되어 자유로운 배포 일정이 가능
     - 실험 정보를 로깅이 가능해서 자유로운 AB 테스트가 가능
     - 최종 결과만 응답하게 되니 시스템 내부 구성을 자유롭게 가능
- (2) 컨테이너 & 쿠버네티스
  - (as-is) IDC 인프라위에서 conda 기반 GPU 학습 
    - 수행 목록
      - 학습 데이터 다운로드
      - 모델 학습
      - 모델 업로드
    - EKS 기반의 airflow 에서는 IDC 인프라로 명령
    - 어떤 문제?
      - 실행 환경 관리의 어려움
      - 독립적인 환경 구성이 없어서 학습 안정성/충돌 문제
  - (to-be) 쿠베 pod 환경 위에서 conda 수행
    - airflow에서는 쿠베 pod 명령어 수행
    - 장점
      - 의존성이 정리
      - 학습 안정성 향상
    - 단점
      - 개발 자유도 하락
        - 서버에 직접 접근해서 할 수 있는 작업들을 다 못하게 됨
      - 러닝 커브
  - 쿠버네티스의 혜택
    - 매일 같이 새로운 서비스가 출시되는 데이터 엔지니어링 생태계에서 k8s는 새로운 환경에 대한 빠르고 자유로운 실험 가능
- (3) 실시간
  - 사용자의 실시간 피드백, 실시간 추천결과, 모델의 성능을 실시간으로 확인하고 싶은 등의 요구사항 발생
  - 다 같은 실시간은 아니다
    - 모델 동작 시점) 실시간 모델 추론
    - 데이터 취득 시점) 실시간 데이터 파이프라인
  - 추천 + 실시간 = 새로운 가능성
    - 방금 본 메뉴, 방금 클릭한 가게, 방금 장바구니에 담은 상품
    - 간단한 방법
      - 서비스 백엔드에서 실시간 데이터를 함께 추천 시스템에 전달하기
      - 추천 시스템팀에서는 할 게 없음
      - 단, 또 팀들간 의존성이 또 생기게 됨
        - 모델의 요구사항이 실시간이 아니라 최근 30초 내로 변경하는 등 모델 요구사항이 변경된다면?
        - 새로운 실시간 데이터가 필요하다면?
        - 실시간 데이터 전달 과정에 문제가 발생한다면?
    - 그래서 직접 수집하자
      - 실시간 클릭 이벤트를 카프카 클러스터 등에서 수집
      - 카프카 커넥터를 통해 MongoDB에 저장
      - 저장된 데이터를 추천 시스템이 사용하여 추천시스템 API에서 추천 상품 전달

우리가 얻은 교훈

- 선택과 집중의 중요성
  - 추천 시스템을 구성하기 위해 필요한 것들이 너무 많음
  - 한정된 인원으로 성과를 내기 위해서는 선택과 집중이 필요
  - EKS를 직접관리 하는 것
    - 직접 하기엔 일정/업무 부담
    - 타 팀의 플랫폼에 의존하기엔 우리 팀 요구사항 수용 여부나 긴급 상황에 대한 대비 등이 가능할까?
    - 결국 타팀의 플랫폼에 의존하기로 함
- 목적을 잊지 말자
  - 계획에 없던 일을 하거나 
  - 목적보다 수단에 집중하거나
  - 길을 잃지 않으려고
    - 목표를 다시 한번 명확하게 설정한다.
    - 빠르게 목표를 달성한다
    - 길을 잃으면 기록하고, 돌아간다, 그리고 나중에 다시 가본다
    

## [14:00 ~ 14:45 트랙 F] 배민스토어 일반셀러 프로젝트 진행기

우리동네에서 만날 수 있는 소상공인 가게 (일반셀러) 프로젝트 시작
  
기존 프로젝트의 문제점
 
- 낮은 코드 품질
  - 코드 커버리지 10%의 적은 테스트 코드
  - 하드코딩
  - 비효율적인 로직
- 전시 서버의 커머스 플랫폼 직접 연동
  - 직접 API를 호출하는 강한 의존성
  - 급격한 트래픽 발생시 장애 전파
- 느린 API 응답 속도

### 프로젝트 실행 전략 5가지

1. 심리적 안정감
2. 목표 설정과 마음가짐
3. 팀 스터디
4. 명확한 역할 부여하기
5. 사내 플랫폼에 올라타기

#### 심리적 안정감

서로 다른 조직에 있던 백엔드 개발자들을 모아 새롭게 만든 팀  
  
그라운드룰 활용
- 팀원들이 함께 일하고 생활하면서 지켜야 할 기본 규칙
- ex) 모르는 것이 있다면 이해될 때까지 질문하자, 같은 질문을 10번을 받더라도, 착실히 답변해주자

#### 목표 설정과 마음가짐

- 목표설정
  - 일반 셀러 도입
  - 전시 아키텍처 개선
  - 전체 API 재개발
- 마음가짐
  - 우리 팀의 첫 번째 큰 도전
    - 어딘가 부딪치더라도 달려보자.
  - 즐기자
    - 최대한 경험하면서 즐기자.
  - 항상 방법은 있다.
    - 현실을 받아들이고, 그 상황에서 최선의 방법을 생각하자.

#### 팀 스터디

프로젝트 기술 검토 후 새 기술을 도입하기로 결정  
단, 팀 스터디를 진행하기로 결정

- 스터디 규칙
  - 매주 화요일 오전 1시간 진행
  - 업무와 관련된 주제
  - 치열하게 진행할 것
  - 기록을 남길 것 
    - 온보딩에 활용
- 스터디 내용
  - 타 팀의 아키텍처를 레퍼런스 삼아 스터디 하기도 함
    - 가게노출 시스템 등
  - Kotlin, Spring Webflux 등 스터디

#### 명확한 역할 부여하기

- 주 담당자/부담당자
- 가장 하고 싶은 부분을 담당하기

### 사내 플랫폼에 올라타기

플랫폼에 연동하기 보다는 필수 기능을 직접 구현하고 배포하는게 더 빨랐음
  
커뮤니케이션과 일정 조율
- 배민 스토어 이해시키기
  - 타 플랫폼 시스템 팀에 우리팀의 서비스를 이해시키기
- 우리에게 필요한 기능 자세히 공유하기
  - 우리 팀에 필요한 기능을 플랫폼 시스템 팀에 공유하기
  - 해당 플랫폼팀의 로드맵 일정에 맞춰서 제안
- 해당 플랫폼의 방향성과 로드맵

### QnA

Q. 레거시를 리팩토링 하는 시기
- 레거시를 개선하는 것은 별도의 시간이 필요하지 않음
- 매번 과제를 하면서 중간 중간 진행
- 기술적 개선이 필요한 부분은 계속 리스트업을 해두고, 과제를 진행하면서 적용

Q. 기술 도입 중 일부 팀원이 받아들이지 않거나 두려워하면
- 새로운 기술이 필요한지 아닌지 팀장이 확인 필요
- 1on1 하면서 팀원의 마음을 계속 들어봐야함
- 새로운 기술에 대한 거부감이 심하면 지켜줄 필요도 있음
- 다만, 절대 다수의 팀원들이 원하면 설득해서 진행

Q. 주니어가 사업과제를 우선하는 리더를 설득할 수 있는 방법
- 사업과제는 되게 중요함
- 이 비즈니스가 잘되지 않으면 팀 자체가 어려워질 수 있음

## [14:55 - 15:35 트랙 D] 우아한 FinOps: 클라우드 비용과 성능 사이

> 발표자가 4분이셨당

### FinOps

목표: 최적화된 클라우드 비용으로 최대의 비즈니스를 창출한다.
- 비용과 품질, 속도 3가지의 균형을 잡는 작업

클라우드 환경으로 넘어오면서 구매/재무의 허가 없이 인프라 구매가 가능해짐  

FinOps 파운데이션 프레임워크 (라이프 사이클) 을 참고하여 실행

- 정보 제공 단계
  - 태깅 및 비즈니스 지표 매핑
  - 부서별 비용 할당 및 비용 예상
- 최적화 단계
  - 사용량과 요율 최적화
  - 사용량 최적화
    - 자원 최적화
    - 최신 리소스 검토 및 적용
  - 단가 최적화
    - RI/SP 계약
    - 신규 할인 프로그램 적용 검토
- 운영 단계
  - 지속적 개선
  - 비즈니스 지표 연결 및 정기 회의
  - 비용 오너십 부여 및 적용 (쇼백, 차지백)
  - FinOps 거버넌스

### FinOps 페르소나

페르소나
- FinOps 실무자
- 엔지니어/인프라 
- 경영진
- 프로덕트 오너
- 재무/구매

FinOps 팀을 중심으로 모든 이해관계자들이 오너십을 가지고 하나의 팀 처럼 협업 하는 것이 중요

#### FinOps 실무자 

- 매출 증가율 보다 클라우드 비용 증가율이 더 높았음
  - 매출이 90~100% 성장하는 동안 클라우드 비용은 100% 초과 증가

AWS CFF 도구 활용
목표
- 클라우드 비용의 상세 분석
- 비즈니스 데이터와 연결 분석
- 식별되지 않은 비용을 최소화

상세 목표
- AWS CFF
  - AWS Cost Explorer
    - 청구 데이터의 요약
    - 데이터 컬럼수 20개
  - AWS Cost and Usage Report (CUR)
    - 청구 데이터의 원본
    - 데이터 컬럼수 297개
  - 이 중 AWS CUR 선택 
    - 태그 기준 인스턴스 타입별로 최근 3개월간 사용량, 비용을 데일리로 보기에 다양한 관점에서 볼 수 있음
    - Cost 최적화를 위한 CUR 쿼리를 17개 카테고리로 제공

AWS CUR 분석만으로는 한계가 있음
- 어떤 태그는 어떤 서비스의 것인가 등등

AWS CUR과 사내 서비스 구성 정보, 팀정보, AWS 예산 등을 함께 사용해서 우리만의 비용 모니터링 구성

비용 가식성 개선
- 출처를 모르는 비용이 많았음
- FinOps 태깅 전략: 비용이 큰 리소스 순으로 적용
  - 최소한의 태깅 공수로 최대의 효과를 볼 수 있음
  - 리소스 갯수 만으로 태깅하게 되면 시간 대비 효과가 적음
- CUR 데이터에는 다양한 정보가 있으니 이를 활용해 태깅 후처리라도 하자

> 각 팀에서 관리하는 리소스에 직접 태깅하는 것이 불가능한가?

AWS CUR -> AWS CUR 수집 -> 태그 없는 리소스 후처리 -> 비즈니스 데이터 Join -> 기타 보정 로직 -> 우아한 클라우드 비용 데이터 
=> Splunk로 관리

클라우드를 잘 쓰고 있는가?
- 클라우드 비용을 쓰는 만큼의 매출 효율을 내고 있는 것인가?
  - 고객 주문 1건당 처리 비용 = AWS 비용 / 주문수
  - 매출 대비 AWS 비용 비율 = AWS 비용 / 서비스 매출
  - 팀 예산 대비 AWS 비용 = 예산 - AWS 비용
    - 차지백을 적용하면 팀 단위로 최적화가 자발적으로 가능

비용 이상 탐지

데이터 기반 비용 최적화
- 개발팀과 FinOps 의 KPI는 다름
  - 서비스 안정성과 품질 개선 vs 비용 최적화
  - 기존 서비스의 품질이 떨어지지 않는다는 것을 증명하며 비용 최적화
- 시간대별 Scale In/Out 의 적정양을 데이터 기반으로 결정
- FinOps 적용할 경우에 대한 예상 적용 결과 공유
  - 연간 얼마의 절감
  - 팀별 AWS 비용 절감양
  - Scale In 시간대 장애가 없을 것이라는 지표

FinOps 실무자라면 바로 최적화부터 들어가지 말고,  
비용 가시성 확보와
클라우드를 현재 잘 쓰고 있는지에 대한 리뷰부터 시작한다.

### 클라우드 최적화 FinOps 활동

서비스 특성
- 트래픽이 특정 시간에 몰림
- 그래서 트래픽이 예측 가능
- 새벽 시간 Scale in 진행

조정 규칙
- CPU, 메세지큐 소화정도 등을 기준으로 desired를 수시로 조정
- 스케줄을 기반으로 min 조정
- 줄일때는 천천히, 늘릴 때는 급격히

스팟 인스턴스
- AWS의 남는 EC2 인스턴스 기간 한정 세일 
  - 언제나 AWS로부터 회수 될 수 있음
  - 중단율 관리가 필요
    - AWS의 공개 자료는 부정확
    - 실시간 중단율 자체 추적

### 재무 관점의 FinOps

- 시장환경 변화에 따라 성장성보다 수익성을 강조하는 시대를 맞이함  
- 시장 환경 악화에도 매출/영업이익 모두 상승하여 사업의 펀더멘탈 증명
  - 2022년 매출 2.95조, 영업이익 4241억 달성

대외적,내재적으로 존재하는 여러 리스크 존재
- 배달비에 대한 부정적 인식
- 시장 경쟁자
- 지속적인 수익성 강화 기조

즉, 한해만 잘 나와서는 안되고 지속가능한지 증명 필요

**클라우드 비용의 특성**
작년 전체 비용의 50%가 클라우드 비용
- 사전 통제 불가
- 비용 예측 어려움
- 예산 타당성 파악 어려움
- 사후 분석 어려움

AWS 항목별 회계인식 기준을 설립하고 확정
- 항목별 breakdown 통한 직관성 상승
- 초과/과소 청구분 확인 가능
- 월별 청구금액 및 손익 안정화

현황파악 / 추정 강화
- 월별 추정 강화
  - 세분화한 항목별 추정
  - 추정 월 1회 -> 2회로 증가
- 일별 비용 트래킹
  - 튀는 항목 확인 및 점검 요청
- 매월 인보이스 세부 점검
  - 청구 항목 매월 확인
  - 필요 시 조정 요청


## [15:50 - 16:15 트랙 G] 인사할 시간도 없습니다. QA의 시간을 아껴줄 테스트 자동화 도입기

실무자 관점 테스트 자동화의 허들
- 프로그래밍에 대한 부담
- 러닝 커브
- 유지보수에 투입 되는 지속적인 리소스
- 낮은 ROI + 우선순위
  
위 문제들을 해결하는 새로운 패러다임
- GPT + Copilot

### API 테스트 자동화

코딩 없이 HttpRequest 코드 작성하기
- Charles Proxy 사용하여 HTTP Request를 cURL로 복사
- 복사된 cURL을 Chat-gpt 프롬프트로 전달
- GPT가 변환한 파이썬 코드를 자동화 프로젝트에 적용

### QAOps

- Jira Rest API를 통해 테스트 실패시 실패한 내역에 대한 자동 JIRA 티켓 생성하기
- CI/CD 파이프라인에서 테스트 레포트 결과를 Slack Webhook으로 전송

## [16:35 - 17:15 트랙 E] 어느 날 시니어가 사라졌다! 주니어 5명의 일 문화 가꾸기

배민 선물하기 팀 내 FE 파트의 이야기
- 시니어 1명, 주니어 5명으로 이루어진 파트
- 시니어 1명이 조직 개편으로 다른 조직으로 이동하게 됨
- 주니어 5명으로 이루어진 팀이 됨

현재 9개월간의 파트/팀 변화 이약.

### 시니어 vs 주니어

스태프 엔지니어 책에서 잉갸ㅣ하는 주니어/시니어 엔지니어의 차이점

- 시니어
  - 팀 내/외 영향력
    - 인사이트 공유
    - 파트 의견 정리
    - 방향성 제시
    - 파트 내/외부의 존재감 발휘
      - 팀원의 성취도 공유 (작은 성취에도 슬랙에서 공유하고 칭찬)
      - 각 팀원의 업무와 기여도 리마인드
      - 파트의 일 문화 알리기
  - 가장 지루하고 시시한 일을 도맡아 하기도 한다
    - 낡은 코드와 레거시를 다루는 일
      - Vue.js & JS로 구현된 레거시 코드를 시니어가 다루는 등
  - 가장 어려운 테크니컬 문제를 풀어내기도 한다.

시니어가 사라진 후의 변화
- 방향성 도출의 어려움
- 팀 내에서 프론트엔드 파트의 존재감이 잘 드러나지 않는다.
- 팀 내/외 커뮤니케이션 비용 증가

### 주니어 엔지니어로서의 노력

해결 방법
- 그라운드룰 정립
  - 자주 질문한다.
  - 페어프로그래밍은 필요한 사람이 먼저 제안한다 등
- 데일리 스레드
  - 매일 오전 9시에 리마인드와 함께 알람 발생
  - 해당 스레드에 오늘 해야할 일을 공유
- 위클리 고도화 및 먼슬리 회고
  - 위클리 시간을 좀 더 알차게 사용하고 싶은 마음 + 파트 회고 니즈
  - 위클리
    - 아이스브레이킹 시간 정하기
    - 논의 안건 쌓아두기
    - 함께 작성하고 함께 이야기하기
    - KPT
  - 먼슬리 회고
    - 구글 잼보드 이용
    - KPT 기반 회고
    - 회고를 통한 액션 아이템 도출
- 요정 문화 도입
  - 그라운드룰을 잘 지켜서 일 문화를 단단하게 유지하고 싶다는 욕구
  - 파트 문화 요정 
    - 파트 내부에서 의견 정리 및 리드 (당번같은?)
    - 하는 일
      - 먼슬리 회고 진행
      - 파트 내 기념일 챙겨주기
      - 그라운드룰 점검
      - 잊고 있던 안건 끌올림
  - 기술 과제 요정
    - 과제 진척도 트래킹
    - 과제 전반 매니징
    - 문서화
- 테크 스펙 도입
  - 기술 과제 문서화
  - 템플릿 사용으로 가독성 높은 문서
  - 체계적인 히스토리 기록

### 부수효과

- 주니어끼리 해결 불가능한 문제
  - 기술 부채 청산에 대한 리소스를 얼마나 투여해야하는지
  - Vue로 만들어진 서비스를 React로 마이그레이션 해야하는데 노하우 부족
- 타 부서의 시니어들의 도움이 필요했던 이유 다시 한번 정리
  - 정리된 내용을 기반으로 사내의 여러 종류의 기술 리더에게 도움을 요청
- 요정 효과
  - 주니어들의 매니징 경험
  - 커뮤니케이션 스킬 향상
  - 전체적인 그림을 볼 수 있는 시야
  - 프로젝트의 각 단계별 고려 사항
- 유대감
  - 주니어 5명간 유대감이 형성됨
  - 이렇게 말해도 기분 나빠하지 않을것 같다는 확신이 있어 좀 더 솔직한 피드백 가능
    - "제가 능력이 부족해서", "내가 너무 사소한것까지 하는 것은 아닐까" 등 극도의 겸소한 발생 하던 문제 해결

### 주니어 엔지니어로서의 한계점

- 경험적 인사이트가 요구되는 문제
- 커리어측 조언이 필요할 때
- 기술적 조언이 필요할 때

이럴때는 파트 밖으로 벗어나서 도움을 요청

### 다시 한번 주니어 vs 시니어

- https://techblog.woowahan.com/2525/

- 시니어 개발자는 팀이 만들어 가는 것
- 그룹에서 원하는 시니어의 조건들을 정확하게 세우고 구성원들과 공감하며 그 기준점에 대해 지향



## [17:30 - 18:10 트랙 G] ElastiCache 운영을 위한 우아한 가이드: 초고속 메모리 분석 툴 개발기와 레디스 운영 노하우 소개

### Elastic Cache

- 주요 파라미터 설정이 인스턴스 타입별로 자동 변경

### 이슈 발생시 확인해야할 것

- CPU
- 메모리
  - Fragmentation이 발생하고 있는건 아닌지
- 네트워크
  - 인바운드/아웃바운드 트래픽 체크
- 커넥션
- 커맨드
- 개발팀

### 이슈 사례

#### 사례 1 

- 지표
  - CPU가 92%까지 튀었음
  - count가 줄어듬
- 체크
  - 어떤 커맨드가 느린지 레이턴시 확인
  - 슬로우 로그 확인
  - 기본 모니터링 지표
    - GetTypeCmds
      - 읽기 전용 유형 명령
    - SetTypeCmds
      - 쓰기 전용 유형 명령
  - 어떤 쿼리가 문제였는지 정확히 확인이 불가능
    - Redis Exporter를 이용한 추가 모니터링 구성
  - 슬로우 로그볼때 주의할 점
    - 단, 레디스는 싱글 스레드라서 실제 처리는 1초지만, 대기 시간에 19초를 쓰면 전체 20초가 수행됨
    - 이러면 슬로우 로그에는 안남을 수 있음 (쿼리는 1초였으니깐)
  - 엔진 CPU 사용율 vs CPU 사용율
    - 엔진 CPU는 90, CPU는 30이면?
      - 엔진: 레디스 프로세스 자체의 로드
      - CPU: 서버 인스턴스의 CPU라 서버에서 사용되는 백그라운드 프로세스까지 포함된 전체 CPU
    - CPU가 4개면: 엔진
    - CPU가 2개 이하면: CPU

#### 사례 2 - 네트워크 대역폭 초과

지표
- CPU가 낮음
- 메모리/ 커넥션 일정
- 네트워크가 높고, 커맨드가 조금 높아짐

원인
- 배포 이후 네트워크 트래픽 급증
- 데이터 아킼텍처 변경
- 클라이언트가 읽어가는 모델 크기 변경
- 네트워크 이슈가 원인임을 찾는데 오래 걸린 이유
  - 대역폭보다 더 높은 네트워크 트래픽을 사용
  - 버스트 대역폭을 사용하더라
- 레디스 특성상 짧은 요청을 빈법ㄴ하게 처리, 순간적으로 동시에 대량 쿼리할 경우
  - 물리적 한계치 넘겨 버스트 대역폭 사용 가능
  - Network bandwidth execced 지표에서 확인 가능
- 버스트 대역폭 사용이 많아 지면서 쿼리 응답속도가 전체적으로 느려진다면
  - 네트워크 성능 부족 의심
  - 스케일 업을 한다
  - 스케일 업 해도 안되면 스케일 아웃을 해서 부하 분산시킨다

### 레디스 메모리 분석 툴 개발

레디스 데이터를 확인하고 싶은 경우가 자주 있음

- 갑자기 메모리가 높아졌을때
- 응답시간이 길어졌을때
- 레디스의 데이터 파악이 필요할 때

레디스 데이터를 가져와서 보는 것 자체가 많은 부하를 일으키기 때문에 운영 서비스에 적용하기가 어려움

신규 프로그램을 사용하면서 매일 배치로 키 분석해서 그라파나 대시보드에 추가해서 확인

#### 구현

키에 대한 분석을 하기 위해 전체 키 리스트 수집
- Scan을 통해 전체 Key를 가져와 for로 조회
- 하나의 키에 대한 정보를 찾기 위해 레디스 3번을 조회하다 보니 성능이 많이 늦음
- 대부분의 지연 시간은 네트워크를 통해 레디스로부터 응답을 받기까지 네트워크 대기 시간이였음

해결
- 파이프라인 방식 도입 
  - pipeline을 통해 배치 사이즈만큼 모아둔 뒤 한번에 수행
- 네트워크 통신을 줄이기 위해 여러 커맨드를 한번에 전송
- 여러개의 쿼리가 한번에 레디스에 전송되기 때문에 네트워크 I/O 시간이 크게 감소
  - 기존 4시간 43분 -> 19분 11초에 처리 완료

하지만, 클러스터 환경에서는 파이프라인 사용할 수 없음
클러스터 환경에서는 데이터가 여러 노드에 분산되어 있어서, 서로 다른 노드에 대한 요청을 처리할 수가 없었음

- 그래서 샤드별로 저장된 키를 조회해서 각각 파이프라인 방식 적용

하지만, **하나의 노드에 저장된 데이터라도 같은 해시슬롯이 아니라면 파이프라인 불가능**

결국, 클러스터 환경에서도 네트워크 I/O를 줄이기 위해 한번에 여러 쿼리를 처리할 수 있는 방법이 없을까..?

#### 루아 스크립트 방식 도입

여러 쿼리를 하나의 스크립트 내에서 수행
- 루아 스크립트를 사용한 배치 처리
- 9분 55초로 더 개선


### QnA

#### 알람 임계치

- 서비스 특성마다 다름
- 레디스를 캐시로 사용하면 메모리 임계치를 
- 레디스에만 데이터를 적재해야 한다면 임계치가 좀 더 높아야함

#### 백업

- 매일 새벽에 백업 수행
- 유지 기간은 칠단


- 오픈소스: https://github.com/woowabros/redis-keys-statistics


## 마무리

- 세션 입장 전 스크린에는 세션 주제가 노출 되었다면 혹시나 잘못입장했을때 미리 알 수 있었을텐데 + 돌아다니면서 방 보고 들어갈 수 있었을것 같은데

